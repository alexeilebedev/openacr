## AMC Intro
<a href="#amc-intro"></a>
Most of software design, or rather design of libraries, looks like this:
library author decides what type of data to represent and manipulate in memory;
Without exception, the library declares some number of *records*, each containing
some fields, and a set of functions that manipulate sets of these records while
maintaining some library-wide invariants.
At a higher level, the library author is said to pick some *data structure*
that offers efficient/economical creation,
modification, updates, and querying. Update operations range from allocation and 
handling of memory-related conditions and errors, to business logic (the most
application-specific part). Sometimes the library is deemed to be purely algorithmic
and not data-oriented. But this is just a viewpoint and doesn't change the correctness
of the above observation. Any computer program maps a set of types from its input domain to an output domain.

Designing abstract data structures is difficult because of the need to track various 
pointers and cross-references in the resulting data structures, and subsequently 
rewriting these pointers without causing corruption. To be robust and to avoid leaks, 
the library must also track resources that were allocated.
This difficulty, and the corresponding debugging effort, which can exceed
the initial coding time by 10- or 100x, means that once a library has been built, it
is not updated frequently. 
Sometimes, the choice of the data structure itself makes adding additional
indexes impractical. For instance, if we think of our main data structure as being 
a "hash table of X", we have committed to a key-value map as the main access structure,
and all other operations are operations other than lookup by key are made less efficient.
And if we have decided to use "a binary tree of X", then we'll use the binary tree lookup
and not consider also hashing the items. This type of thinking characterizes "data structure" 
design, and is taught in schools. 
The very term "data structure" presupposes that we take our data and then structure
it somehow. The metaphor is some sort of data origame, or moving data around for 
better access. There is no concept of access as being separate from the data.
This is assumed to be the price to pay if we want to write good algorithms.

On the other hand, there is a different approach to handling data -- the database approach.
If the data structure approach blends data with indexing (it's inherent in the notion of
"structuring the data"), the database approach maximally separates
them. The data which we want to handle is first split (factored) into various tables.
A table contains a number of records, and each record has some fields (also known as attributes).
We then create *indexes on attributes*, so that we can quickly locate
any record. Typically, any number of such indexes are allowed by a database management system.
Then, we write a query in some special query language, and a mechanism known as the query
planner decides which indexes to use to answer the query (so it's not just a multiply-nested
for-loop over all tables) and hopefully completes the query. But not *that*
quickly. It is an accepted fact that you pay for the generality of this approach with
correspondingly low runtime performance. When you really want performance, you use classical
methods to get it.

And yet, we know that all programming can be viewed as manipulation of in-memory databases.
Thus, the main motivation of *amc* is to give you the flexibility of crafting your own 
in-memory database with any number of tables and indexes, without having to implement
all the operations that implement creation, replacement, update, deletion and querying
of these records, while getting performance that meets and exceeds that of the data structure
approach.

As a software architect and user of amc, you decide on the schema to use for your in-memory
database, and amc follows the constraints in your schema to implement cascading deletes, 
automatic group-bys, fast allocation of records using chained memory pools, conversion of data records
to various formats, and many other things.
The number and the variety of structures generated by amc is more similar to that of
a standard C++ library than an RDBMS.

Amc generates roughly 20 lines of code for each line of input. The code it generates 
is usually of the most error-prone and performance-critical kind, and you can account
for the generated code at the assembly instruction level: every generated function
is written out in full and given a unique key, making it easy to both set breakpoints and
backtrack any potential issues back to the generator. The generated code is documented,
readable and debuggable, and is provided in a standard target language: a conservative subset of C++ 
(circa C++ 1997), eschewing all modern constructs, which exist mainly to reduce the amount
of manual typing. This value proposition has immense consequences:
it means that a large and expensive software project can become a medium-sized software project,
and a medium-sized software project can become a small software project. Programs
can be refactored directly from the command line, by adding and dropping indexes, writing
meta-programs that output amc tables that then become code, and even optimizing programs
programmatically. Ultimately, this is what makes amc worth learning.

When you co-evolve the schema together with your application, you can easily modify your
in-memory structures on the fly, and fine-tune them later.
At the same time, you can co-evolve amc itself with the project.
That's because amc isn't built and installed on the host system as a stand-alone, 
binary package. It is provided in source code form and versioned together with the
project it supports, forming a single eco-system.

### Why Generate?
<a href="#why-generate-"></a>

Good algorithms and data structures for most problems are known.
The problem is attaching them to an application. Usually the costs associated
with using algorithms are:

* Performance cost and complexity cost when using libraries.
* Difficulty understanding symbol renamings (this happens with the C++ templates or when using macro preprocessors).
  Programmer's attention is a finite resource. When the programmer spends this attention in order
  to understand what a program does, he can no longer confirm its correctness, and the program's
  runtime behavior becomes more surprising. Another word for surprise is bug. So when a program doesn't
  look trivial, this leads to more bugs.
* Debugging and reliability cost when hand-coding algorithms.
* Maintenance cost due to having too many lines of code.
* Unexpected changes in upstream generic libraries.

#### Libraries Vs. Custom Code
<a href="#libraries-vs--custom-code"></a>

The motivation for writing generators is that writing code for reusability doesn't work.
The reason it doesn't work is that the definition of correctness doesn't lie with
the piece of reusable code -- it lies with the final application. And so the reusable
code always offers to the application too much and at the same time not enough.
You need a singly linked list, but the library uses a doubly linked list. You need an extra index,
but the library author didn't anticipate it. You have your own strategy for memory
management, but the library insists on its own. And you can't customize the library, since
for every feature you need to change, there is already some user out there who needs it to stay the same.
When you update to the next version of the library, you get, almost by definition, features you didn't ask for.
(Since you were already making do with the previous set of features).
So, code written for reusability never reaches its intended potential in terms
of either performance or utility.

Leaving aside reusability for a moment, as can be seen from real life examples,
all high-performance systems are hand-coded due to highly
specific requirements, and because it allows writing only what's needed, debugging that,
and leaving it to run and do its job indefinitely. Yet hand-coding is difficult and
requires a lot of debugging and chasing of memory errors and dangling pointers.

All of this may not matter when the problems are small and requirements negligible, but it
really starts to matter when you are pushing against the limits of a single machine. The difference
between code that runs in 1 millisecond and 10 milliseconds is eventually the difference between
10 servers and 100 servers.

Thus we have a mini-max problem, which is the first sign of a problem worth solving.
On one hand, we want maximally customizable code that does only what we want.
On the other hand, we want to write and debug the minimal amount of code.

### Exponential Cost Of Software
<a href="#exponential-cost-of-software"></a>

Software complexity models such as [COCOMO](https://en.wikipedia.org/wiki/COCOMO)
estimate the defect rate to grow as an
exponential function of number of source lines. This exponent should not be underestimated.
It is about 1.2 for a national stock exchange with real-time and embedded constraints, meaning that
to write 1,000 lines of code, it costs you 3,981 units of effort.
And if you write 100,000 lines of code, you pay a full 1,000,000 units. This exponential
nature of the cost of lines of code is closely related to the cost of borrowing money.
You basically have to pay back with interest, making every line of code
 in a very real sense "technical debt".

Massive code bases can slow development to a crawl and destroy projects.
I don't want to present a macabre listing of all the software projects that failed
because they had too much code; I will just assume that we all agree on the validity
of the following equation:

    (Project Goodness) = W*(Features) - A*(Amount of Code Written)^B

In other words, we like the features of our system, with some weight `W`,
and we pay with interest `B` and weight `A` for the amount
of code we wrote to get these features.

### AMC Features
<a href="#amc-features"></a>
AMC Generates

* hash tables
* arrays
* linked lists
* dequeues
* binary heaps
* trees
* region-based memory allocators of several types: free list, powers-of-two freelist, fifo allocator, sbrk
* attaching huge pages to any allocator
* fifos
* linear arrays (vectors)
* indirect arrays (with permanent pointers)
* inline arrays and fixed-size arrays
* Functions to convert any struct to/from a string or a bash command line
* Enums based on any column from a table, supporting both integer and string values
* presence masks
* big-endian fields
* Scaled decimal types
* enforced packing for protocol namespaces
* calculation of cryptographic hashes of protocols, ensuring binary compatibility of messages sent over the wire
* parsing of command lines with arbitrary user-defined types, supporting enums, arrays and optional fields
* optimized 'placement new' functions for various memory buffers
* buffers with or without asynchronous I/O, with automatic parsing of messages and edge-triggered epoll/kqueue
* automatic incremental group-by indexes (inserting a record automatically updates global and partitioned indexes)
* syntax help string for an executable
* '#include' lines only for those files that are needed to compile a unit
* forward declarations for any referenced types whose headers don't need to be included
* controlled order of initialization (an unsolved issue in C++)
* field-wise constructors for any record
* copy protection for records which are cross-referenced and thus cannot be moved
* functions to map strings to enums and enums to strings
* allocating arbitrary fields as bitfields within other fields, with checks for overlapping
* getters and setters for any field, if requested
* functions to create a structured pkey out of its components (_Concat functions)
* recursive checking that a packed type doesn't include an unpacked type by value; other consistency checks
* conversion of any type to/from JSON object
* full set of C++ comparison operators for any types
* << and >> operators
* tracking of pointers with automatic cascade delete
* protection against linear scanning when deleting elements of singly linked lists
* lexicographic sorting and comparison of structs with any number of fields, enabling structured keys and multi-dimensional priority queues.
* sort functions on custom fields
* scheduling constructs (steps) for real-time modules
* cycle accounting ('traces') for steps, dispatches
* records indexed with an arbitrary number of indexes (multiple priority queues, linked lists, etc.)
* custom pool (allocator) for any type
* custom pool for any pool, allowing use of completely separate memory for different types (this is used both for performance and security)
* optional alloc/delete counters for any pool
* C++ symbols from ssimfile columns
* Statically loaded tables (that become part of the source code)
* Synchronous and asynchronous subprocess invocation
* Asynchronous I/O functions
* Bitsets on top of any integer or array type
* Char sets, both bitmap-based and computed
* Fixed string types (Pascal strings, Left-padded strings, Right-padded strings, Fixed-length strings) with optional numeric conversion
* Bidirectional mappings between enumerated types
* Dispatches (any group of ctypes), whether sharing a common header with a type field, or not.
* Printing, reading, calling dispatches given both binary and text input
* Uniform cursors (iterators):
** cursors for all indexes
** destructive cursors
** cursors that allow deletion of elements while scanning
** cursors for any defined message types
** cursors for lines in a file, or in a memory region
** cursors for files in directory
** cursors for messages in a memory region.

For each namespace, all code is generated from scratch.
The resulting code forms a *database of source code*,
containing a superset of functions that will be used by the final
application. The generated code is verbose, user-readable, commented,
is intended to be readable by a human, corresponds directly to the final assembly
code with no ambiguity, and intentionally uses only a small, conservative subset of C++. 
`Amc` does not modify or reverse-engineer user code, and it's not a framework
where you have to "plug in" anything. User controls what functions will be called and when.
Thus, `amc` is a tool for constructing robust software based on your specifications.

The authors have spent many years using template-based libraries and
class hierarchies, and found them to be unmaintainable in the long
run.  amc represents a step forward by stepping away from these
concepts; The templates, which are a form of compile-time code
generation, are fully superseded by explicit code generation, which
can perform deductive reaasoning on closed sets, something that's not
available to templates. Templates, which are based on the idea of
substitution of expressions, cannot in princple insert or remove
fields at runtime, or delete certain functions, or create multiple
symbols, and are hard to debug because they break the mapping between
lines of code and assembly, which is required for debugging system
code. Annotated, generated code overcomes this while eliminating an
entire complex sublanguage. Virtual functions, or function pointers,
can be useful and even unavoidable, (and are implemented in amc as
Hooks), but they negatively affect process reliability. Dijkstra's
predicate transformer, which is the only available theoretical tool
for reasoning about imperative software, requires that each action has
known pre- and post-conditions. A virtual call always leaves the
possibility open that the target function will do something that
violates the post-condition, which makes the calling code statically
unanalyzable. That's why our approach is to construct lists or
priority queues of "todo" items in a process, and then process them
with a well-defined loop or step. This forces distinct cases to be
enumerated by some state field or an enum, and collects the code for
processing the distinct cases into one place, making it possible to think
about post-conditions again.

`Amc` loads about 100 ssim tables. The full list can be obtained with
`acr_in amc` and appears at the end of this document.
The exact actual `amc` input can be printed with `acr_in
amc -data`.  About 20% of these tables are responsible for 80% of the
generated code, the rest deal with finer details.

`Amc` was initially coded by hand, but once its capabilities became
powerful enough, it was used to generate data structures for its next
version. As a result, all of `Amc`'s internal data structures, both
input, and computational (internal), are defined as ssim tuples and can be
queried with `acr ns:amc -xref`. The tool is thus unique in that it
generates most of its own source code. Previous approaches focused
either on interpreting the interpreter (LISP) or compiling the
compiler. Generating a source-generator is a first as far as we know.

### Ratio of Generated To Manual LOC
<a href="#ratio-of-generated-to-manual-loc"></a>

On average, `amc` generates 15-25 lines of code per 1 line of ssimfile specification.
It is easy enough to check this claim:

    $ acr ns:amc -t | wc -l
    2010
    $ amc amc.% | wc -l
    47431

The specification can be edited manually and adjusted frequently with Unix tools such as
sed and perl, or by issuing `acr_ed` commands. The compression factor, and the fact
that ssim lines are relatively independent of each other makes the cost of
ssim specifications much lower than the cost of regular code.

### Reading Code of Amc Itself
<a href="#reading-code-of-amc-itself"></a>

Amc source code is located under cpp/amc. The list of all the source files and headers
can also be examined with `acr targsrc:amc/%`

amc input tables are in `data/amcdb` and `data/dmmeta`; The full list can be obtained
with `acr_in amc`.

### Field Name Prefix
<a href="#field-name-prefix"></a>

The functions amc generates are all global functions. I.e. they are not member
functions of any class. The function name is usually built from the name of the related
field, plus a name. For example, a big-endian u32 called `value` will  cause a function
named `value_Get` to be generated. A linked list field called `zd_target` will cause
amc to create functions `zd_target_Insert`, `zd_target_Prev`, etc. Each generated function
is potentially different, and customized just for that particular field. AMC considers the
context of the record (struct) where the field is defined, and references and cross-references between
this record and other records.

### The Algorithm For Generating Any Code
<a href="#the-algorithm-for-generating-any-code"></a>

The main algorithm for generating any code (not just with amc) is simple:

1. Manually write the code that you want to generate and make sure it works.
2. Put a print statement around each line.
3. Move the resulting code to the appropriate place in your generator.
4. Run your generator. Now you have 2 copies of your code: one you started with, and the
  other that was printed by the generator. If you did everything right, you should get a
  "multiply defined" link error now.
5. Delete the manually written code.
6. Parameterize the generator so that it can generate a whole family of implementations that
  consider various cases.

It is usually not a good idea to start programming new features in amc itself.
It is very tiresome to debug such code. The code should already have been written
by hand, possibly a couple of times, to the point where the duplication occurs, but the
different implementations cannot be unified because of either unacceptable performance costs,
or too many dependencies. Such code is to be lifted into a generator.

